\chapter{Example Applications}

This chapter contains a collection of short code snippets and algorithms using
the Bitmanip extension. It also contains some examples of bit manipulation
code that doesn't require any extension beyond the base ISA.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Basic Bitmanipulation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Bitfield extract}

Extracting a bit field of length {\tt len} at position {\tt pos} can be done using
two shift operations.

\begin{verbatim}
  slli a0, a0, (XLEN-len-pos)
  srli a0, a0, (XLEN-len)
\end{verbatim}

Or using {\tt srai} for a signed bit-field.

\begin{verbatim}
  slli a0, a0, (XLEN-len-pos)
  srai a0, a0, (XLEN-len)
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Parity check}

The parity of a word (xor of all bits) is the LSB of the population count.

\begin{verbatim}
  pcnt a0, a0
  andi a0, a0, 1
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Rotate shift of bytes and half-words}

Rotate right shift of the byte in {\tt a0} by the shift amount in {\tt a1},
assuming {\tt a0} is stored in zero-extended form:

\begin{verbatim}
  orc8 a0, a0
  ror a0, a1
  andi a0, a0, 255
\end{verbatim}

And rotate right shift of the 16-bit half-word in {\tt a0} by the shift amount in {\tt a1},
assuming {\tt a0} is stored in zero-extended form:

\begin{verbatim}
  orc16 a0, a0
  ror a0, a1
  pack[w] a0, a0, zero
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Rank and select}

Rank and select are fundamental operations in succinct data structures~\cite{SelectX86}.

\texttt{select(a0, a1)} returns the position of the \texttt{a1}th set bit in \texttt{a0}.
It can be implemented efficiently using \texttt{bdep} and \texttt{ctz}:

\begin{minipage}{\linewidth}
\begin{verbatim}
  select:
    sbset a1, zero, a1
    bdep a0, a1, a0
    ctz a0, a0
    ret
\end{verbatim}
\end{minipage}

\texttt{rank(a0, a1)} returns the number of set bits in \texttt{a0} up to and
including position \texttt{a1}.

\begin{minipage}{\linewidth}
\begin{verbatim}
  rank:
    not a1, a1
    sll a0, a1
    pcnt a0, a0
    ret
\end{verbatim}
\end{minipage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{OR/AND/XOR-reduce in byte vectors}

OR-ing the bytes in a register and returning the resulting byte is
easy with GORC:

\begin{minipage}{\linewidth}
\begin{verbatim}
  gorci a0, a0, -8
  andi a0, 255
\end{verbatim}
\end{minipage}

AND-ing can accomplished by applying De Morgan's laws:

\begin{minipage}{\linewidth}
\begin{verbatim}
  not a0, a0
  gorci a0, a0, -8
  not a0, a0
  andi a0, 255
\end{verbatim}
\end{minipage}

XOR-ing can be accomplished with CLMUL (see also section~\ref{invxorshift}).

\begin{minipage}{\linewidth}
\begin{verbatim}
  andi a1, zero, 0x80
  gorci a1, a1, -8
  clmulr a0, a0, a1
  andi a0, 255
\end{verbatim}
\end{minipage}

Where the first two instructions (andi+gorci) just create the constant 0x8080..8080.

Finally, on RV64, XOR-ing the bytes in a register can also be accomplished with BMATXOR:

\begin{minipage}{\linewidth}
\begin{verbatim}
  andi a1, zero, 0xff
  bmatxor a0, a1, a0
\end{verbatim}
\end{minipage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Counting trailing non-zero bytes}

Counting the trailing (LSB-end) non-zero bytes in a word
is a helpful operation in optimized implementations of {\tt strlen()}
and {\tt strcpy()}:

\begin{minipage}{\linewidth}
\begin{verbatim}
  int count_trailing_nonzero_bytes(long x)
  {
    return _rv_ctz(~_rv_orc_b(x)) >> 3;
  }
\end{verbatim}
\end{minipage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Finding bytes of certain values}

Finding zero bytes is a useful operations for {\tt strchr()}
and {\tt memchr()}:

\begin{minipage}{\linewidth}
\begin{verbatim}
  bool check_zero_bytes(long x)
  {
    return ~_rv_orc_b(x) != 0;
  }
\end{verbatim}
\end{minipage}

To find other bytes we simply XOR the value with a mask of the byte value we
are looking for:

\begin{minipage}{\linewidth}
\begin{verbatim}
  bool check_byte(long x, unsigned char c)
  {
    return ~_rv_orc_b(x ^ _rv_orc8(c)) != 0;
  }
\end{verbatim}
\end{minipage}

These schemes can easily be extended with {\tt ctz} and {\tt pcnt} to perform
operations such as counting the number of bytes of a certain value within a
word, or finding the position of the first such byte.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Fill right of most significant set bit}

The ``fill right'' or ``fold right'' operation is a pattern commonly used in bit manipulation code.~\cite{MAGIC}

The straight-forward RV64 implementation requires 12 instructions:

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint64_t rfill(uint64_t x)
  {
    x |= x >> 1;   // SRLI, OR
    x |= x >> 2;   // SRLI, OR
    x |= x >> 4;   // SRLI, OR
    x |= x >> 8;   // SRLI, OR
    x |= x >> 16;  // SRLI, OR
    x |= x >> 32;  // SRLI, OR
    return x;
  }
\end{verbatim}
\end{minipage}

With {\tt clz} it can be implemented in only 4 instructions. Notice the
handling of the case where {\tt x=0} using {\tt sltiu+addi}.

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint64_t rfill_clz(uint64_t x)
  {
    uint64_t t;
    t = clz(x);         // CLZ
    x = (!x)-1;         // SLTIU, ADDI
    x = x >> (t & 63);  // SRL
    return x;
  }
\end{verbatim}
\end{minipage}

Alternatively, a Trailing Bit Manipulation (TBM) code pattern can be used
together with {\tt rev} to implement this function in 4 instructions:

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint64_t rfill_rev(uint64_t x)
  {
    x = rev(x);         // GREVI
    x = x | ~(x - 1);   // ADDI, ORN
    x = rev(x);         // GREVI
    return x;
  }
\end{verbatim}
\end{minipage}

Finally, there is another implementation in 4 instructions using BMATOR, if we do
not count the extra instructions for loading utility matrices.

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint64_t rfill_bmat(uint64_t x)
  {
    uint64_t m0, m1, m2, t;

    m0 = 0xFF7F3F1F0F070301LL;  // LD
    m1 = bmatflip(m0 << 8);     // SLLI, BMATFLIP
    m2 = -1LL;                  // ADDI

    t = bmator(x, m0);          // BMATOR
    x = bmator(x, m2);          // BMATOR
    x = bmator(m1, x);          // BMATOR
    x |= t;                     // OR

    return x;
  }
\end{verbatim}
\end{minipage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Round to next power of two}

One common application of {\tt rfill()} is rounding up to the next power of two:

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint64_t round_pow2(uint64_t x)
  {
    return rfill(x-1)+1;
  }
\end{verbatim}
\end{minipage}

This can also be implemented in just 4 instructions, if we don't care about the
case where the above code overflows because {\tt x} is already larger than the
largest power-of-two representable in an {\tt uint64\_t}.

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint64_t round_pow2(uint64_t x)
  {
    uint64_t t;
    t = clz(x-1);     // ADDI, CLZ
    x = ror(!!x, t);  // SLTU, ROR
    return x;
  }
\end{verbatim}
\end{minipage}

Note that this code handles $0\rightarrow{}0$ and $1\rightarrow{}1$ correctly,
i.e. equivialent to {\tt rfill(x-1)+1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Packed vectors}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Packing bytes}

The following RV32 code packs the lower 8 bits from a0, a1, a2, a3 into
a 32-bit word returned in a0, ignoring other bits in the input values.

\begin{minipage}{\linewidth}
\begin{verbatim}
  packh a0, a0, a1
  packh a1, a2, a3
  pack  a0, a0, a1
\end{verbatim}
\end{minipage}

And the following RV64 code packs 8 bytes into a register.

\begin{minipage}{\linewidth}
\begin{verbatim}
  packh a0, a0, a1
  packh a1, a2, a3
  packh a2, a4, a5
  packh a3, a6, a7
  packw a0, a0, a1
  packw a1, a2, a3
  pack  a0, a0, a1
\end{verbatim}
\end{minipage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Permuting bytes}

There are 24 ways of arranging the four bytes in a 32-bit word. {\tt ror}, {\tt
grev}, and {\tt [un]shfl} can perform any of those permutations in at most 3
instructions. Table~\ref{permbytes} lists those sequences.~\cite{Wolf19A}

\begin{table}[h!]
\begin{center}
\begin{tabular}{c|l}
Bytes & Instructions \\
\hline
A B C D & {\it initial byte order} \\
A B D C & {\tt ROR(24),SHFL(8),ROR(8)} \\
A C B D & {\tt SHFL(8)} \\
A C D B & {\tt ROR(8),GREV(8),SHFL(8)} \\
A D B C & {\tt ROR(16),SHFL(8),ROR(24)} \\
A D C B & {\tt ROR(8),GREV(8)} \\
\hline
B A C D & {\tt ROR(8),SHFL(8),ROR(24)} \\
B A D C & {\tt GREV(8)} \\
B C A D & {\tt ROR(16),SHFL(8),ROR(8)} \\
B C D A & {\tt ROR(24)} \\
B D A C & {\tt GREV(8),SHFL(8)} \\
B D C A & {\tt ROR(24),SHFL(8)} \\
\hline
C A B D & {\tt ROR(8),GREV(24),SHFL(8)} \\
C A D B & {\tt ROR(16),SHFL(8)} \\
C B A D & {\tt ROR(8),GREV(24)} \\
C B D A & {\tt SHFL(8),ROR(24)} \\
C D A B & {\tt ROR(16)} \\
C D B A & {\tt ROR(8),SHFL(8),ROR(8)} \\
\hline
D A B C & {\tt ROR(8)} \\
D A C B & {\tt SHFL(8),ROR(8)} \\
D B A C & {\tt ROR(8),SHFL(8)} \\
D B C A & {\tt GREV(24),SHFL(8)} \\
D C A B & {\tt ROR(24),SHFL(8),ROR(24)} \\
D C B A & {\tt GREV(24)} \\
\end{tabular}
\end{center}
\caption{Instruction sequences for arbitrary permutations of bytes in a 32-bit word.}
\label{permbytes}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Widening and narrowing}

The {\tt [un]zip} instructions can help with widening and narrowing packed
vectors. For example, narrowing the bytes in two words into a single
word with the values in nibbles with values from {\tt a0} in LSB half and
values from {\tt a1} in MSB half:

\begin{minipage}{\linewidth}
\begin{verbatim}
  unzip4 a0, a0
  unzip4 a1, a1
  pack a0, a0, a1
\end{verbatim}
\end{minipage}

And widening the nibbles from {\tt a0} into bytes in {\tt a1} (MSB half)
and {\tt a0} (LSB half), with zero extension:

\begin{minipage}{\linewidth}
\begin{verbatim}
  srli a1, a0, XLEN/2
  pack a0, a0, zero
  zip4 a1, a1
  zip4 a0, a0
\end{verbatim}
\end{minipage}

And finally the same widening operation with sign extension:

\begin{minipage}{\linewidth}
\begin{verbatim}
  addi t0, zero, 8
  orc4 t0, t0
  and t0, t0, a0
  orc.n t0, t0
  srli t1, t0, XLEN/2
  srli a1, a0, XLEN/2
  pack a1, a1, t1
  pack a0, a0, t0
  zip4 a1, a1
  zip4 a0, a0
\end{verbatim}
\end{minipage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Shifting packed vector elements}

Using {\tt zip} we can re-arrange the bits in a packed vector of $N$ elements
so that a shift by $k$ of each byte becomes a shift of $Nk$ of the entire new
vector. So we zip, shift, and then {\tt unzip} to shuffle everything back. The number
of {\tt zip} and {\tt unzip} is $\textrm{log}2(N)$. This works for all kinds of
shift operations. For example, rotating a vector of bytes on RV32 in 6
instructions:

\begin{minipage}{\linewidth}
\begin{verbatim}
  zip a0, a0
  zip a0, a0
  slli a1, a1, 2
  ror a0, a0, a1
  unzip a0, a0
  unzip a0, a0
\end{verbatim}
\end{minipage}

Because {\tt zip; zip; zip} is equal to {\tt unzip; unzip} on RV32,
and equal to {\tt unzip; unzip; unzip} on RV64, we need never more
than 2 {\tt [un]zip} on RV32, or 3 {\tt [un]zip} on RV64.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Adding packed vectors}

The following six instructions will add the elements of the two vectors passed
in {\tt a0} and {\tt a1}, and return the vector of sums in {\tt a0}.

This expects a mask in {\tt a2} that marks the MSB bit of each vector
element. For a vector of bytes this mask would be {\tt 0x8080...80} (which
can be obtained in two instructions via {\tt orc8(0x80)}).

\begin{minipage}{\linewidth}
\begin{verbatim}
  xor  a3, a0, a1
  and  a3, a3, a2
  andn a0, a0, a2
  andn a1, a1, a2
  add  a0, a0, a1
  xor  a0, a0, a3
\end{verbatim}
\end{minipage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Funnel shifts}
\label{funnel}

A funnel shift takes two XLEN registers, concatenates them to a $2 \times
\textrm{XLEN}$ word, shifts that by a certain amount, then returns the lower
half of the result for a right shift and the upper half of the result for a
left shift.

The {\tt fsl}, {\tt fsr}, and {\tt fsri} instructions perform funnel shifts.

\subsection{Bigint shift}

A common application for funnel shifts is shift operations in bigint libraries.

For example, the following functions implement rotate-shift operations
for bigints made from {\tt n} XLEN words.

\begin{minipage}{\linewidth}
\begin{verbatim}
  void bigint_rol(uint_xlen_t data[], int n, int shamt)
  {
    if (n <= 0)
      return;

    uint_xlen_t buffer = data[n-1];
    for (int i = n-1; i > 0; i--)
      data[i] = fsl(data[i], shamt, data[i-1]);
    data[0] = fsl(data[0], shamt, buffer);
  }

  void bigint_ror(uint_xlen_t data[], int n, int shamt)
  {
    if (n <= 0)
      return;

    uint_xlen_t buffer = data[0];
    for (int i = 0; i < n-1; i++)
      data[i] = fsr(data[i], shamt, data[i+1]);
    data[n-1] = fsr(data[n-1], shamt, buffer);
  }
\end{verbatim}
\end{minipage}

These version only works for shift-amounts $<$XLEN. But functions supporting
other kinds of shift operations, or shifts $\ge$XLEN can easily be built
with {\tt fsl} and {\tt fsr}.

\subsection{Parsing bit-streams}

The following function parses {\tt n} 27-bit words from a packed array of XLEN words:

\begin{minipage}{\linewidth}
\begin{verbatim}
  void parse_27bit(uint_xlen_t *idata, uint_xlen_t *odata, int n)
  {
    uint_xlen_t lower = 0, upper = 0;
    int reserve = 0;

    while (n--) {
      if (reserve < 27) {
        uint_xlen_t buf = *(idata++);
        lower |= sll(buf, reserve);
        upper = reserve ? srl(buf, -reserve) : 0;
        reserve += XLEN;
      }
      *(odata++) = lower & ((1 << 27)-1);
      lower = fsr(lower, 27, upper);
      upper = srl(upper, 27);
      reserve -= 27;
    }
  }
\end{verbatim}
\end{minipage}

And here the same thing in RISC-V assembler:

\begin{minipage}{\linewidth}
\begin{verbatim}
  parse_27bit:
    li t1, 0              ; lower
    li t2, 0              ; upper
    li t3, 0              ; reserve
    li t4, 27             ; shamt
    slo t5, zero, t4      ; mask
    beqz a2, endloop      ; while (n--)
  loop:
    addi a2, a2, -1
    bge t3, t4, output       ; if (reserve < 27)
    lw t6, 0(a0)                 ; buf = *(idata++)
    addi a0, a0, 4
    sll t7, t6, t3               ; lower |= sll(buf, reserve)
    or t1, t1, t7
    sub t7, zero, t3             ; upper = reserve ? srl(buf, -reserve) : 0
    srl t7, t6, t7
    cmov t2, t3, t7, zero
    addi t3, t3, 32              ; reserve += XLEN;
  output:
    and t6, t1, t5           ; *(odata++) = lower & ((1 << 27)-1)
    sw t6, 0(a1)
    addi a1, a1, 4
    fsr t1, t1, t2, t4       ; lower = fsr(lower, 27, upper)
    srl t2, t2, t4           ; upper = srl(upper, 27)
    sub t3, t3, t4           ; reserve -= 27
    bnez a2, loop         ; while (n--)
  endloop:
    ret
\end{verbatim}
\end{minipage}

A loop iteration without fetch is 9 instructions long, and a loop iteration
with fetch is 17 instructions long.

Without ternary operators that would be 13 instructions and 22 instructions,
i.e. assuming one cycle per instruction, that function would be about 30\%
slower without ternary instructions.

\subsection{Fixed-point multiply}

A fixed-point multiply is simply an integer multiply, followed by a right
shift. If the entire dynamic range of XLEN bits should be useable for the
factors, then the product before shift must be 2*XLEN wide. Therefore {\tt
mul}+{\tt mulh} is needed for the multiplication, and funnel shift instructions
can help with the final right shift. For fixed-point numbers with N fraction
bits:

\begin{minipage}{\linewidth}
\begin{verbatim}
  mul_fracN:
    mulh a2, a0, a1
    mul a0, a0, a1
    fsri a0, a0, a2, N
    ret
\end{verbatim}
\end{minipage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Arbitrary bit permutations}

This section lists code snippets for computing arbitrary bit permutations that
are defined by data (as opposed to bit permutations that are known at compile
time and can likely be compiled into shift-and-mask operations and/or a few
instances of bext/bdep).

\subsection{Using butterfly operations}
\label{butterfly}

The following macro performs a stage-{\tt N} butterfly operation on the word in
{\tt a0} using the mask in {\tt a1}.

\begin{verbatim}
  grevi a2, a0, (1 << N)
  cmix a0, a1, a2, a0
\end{verbatim}

The bitmask in {\tt a1} must be preformatted correctly for the selected butterfly
stage. A butterfly operation only has a XLEN/2 wide control word. The following
macros format the mask assuming those XLEN/2 bits in the lower half of {\tt a1}
on entry:

\begin{verbatim}
bfly_msk_0:
  pack a1, a1, a1
  zip a1, a1

bfly_msk_1:
  pack a1, a1, a1
  zip2 a1, a1

bfly_msk_2:
  pack a1, a1, a1
  zip4 a1, a1

...
\end{verbatim}

A sequence of $2\cdot{}log_2(\textrm{XLEN})-1$ butterfly operations can perform any
arbitrary bit permutation (Bene{\v s} network):

\begin{verbatim}
  butterfly(LOG2_XLEN-1)
  butterfly(LOG2_XLEN-2)
  ...
  butterfly(0)
  ...
  butterfly(LOG2_XLEN-2)
  butterfly(LOG2_XLEN-1)
\end{verbatim}


Many permutations arising from real-world applications can be implemented
using shorter sequences. For example, any sheep-and-goats operation (SAG, see section~\ref{SAG})
with either the sheep or the goats bit reversed can be implemented in $log_2(\textrm{XLEN})$
butterfly operations.

Reversing a permutation implemented using butterfly operations is as simple as
reversing the order of butterfly operations.

% References
% http://www.princeton.edu/~rblee/PUpapers/xiao_spie00.pdf
% https://www.lirmm.fr/arith18/papers/hilewitz-PerformingBitManipulations.pdf
% https://pdfs.semanticscholar.org/bcd0/8fdccf3d5ab959fd81162bd811706ba1676a.pdf

\subsection{Using omega-flip networks}

The omega operation is a stage-0 butterfly preceded by a zip operation:

\begin{verbatim}
  zip a0, a0
  grevi a2, a0, 1
  cmix a0, a1, a2, a0
\end{verbatim}

The flip operation is a stage-0 butterfly followed by an unzip operation:

\begin{verbatim}
  grevi a2, a0, 1
  cmix a0, a1, a2, a0
  unzip a0, a0
\end{verbatim}

A sequence of $log_2(\textrm{XLEN})$ omega operations followed by
$log_2(\textrm{XLEN})$ flip operations can implement any arbitrary 32 bit
permutation.

As for butterfly networks, permutations arising from real-world applications
can often be implemented using a shorter sequence.

% References
% https://ieeexplore.ieee.org/document/878264/
% https://www.princeton.edu/~rblee/ELE572Papers/lee_slideshotchips2002.pdf

\subsection{Using baseline networks}

Another way of implementing arbitrary 32 bit permutations is using a
baseline network followed by an inverse baseline network.

A baseline network is a sequence of $log_2(\textrm{XLEN})$ butterfly(0)
operations interleaved with unzip operations. For example, a 32-bit
baseline network:

\begin{verbatim}
  butterfly(0)
  unzip
  butterfly(0)
  unzip.h
  butterfly(0)
  unzip.b
  butterfly(0)
  unzip.n
  butterfly(0)
\end{verbatim}

An inverse baseline network is a sequence of $log_2(\textrm{XLEN})$ butterfly(0)
operations interleaved with zip operations. The order is opposite to the
order in a baseline network. For example, a 32-bit inverse baseline network:

\begin{verbatim}
  butterfly(0)
  zip.n
  butterfly(0)
  zip.b
  butterfly(0)
  zip.h
  butterfly(0)
  zip
  butterfly(0)
\end{verbatim}

A baseline network followed by an inverse baseline network can implement
any arbitrary bit permutation.

% References
% https://dl.acm.org/citation.cfm?id=1311797

\subsection{Using sheep-and-goats}
\label{SAG}

The Sheep-and-goats (SAG) operation is a common operation for bit permutations.
It moves all the bits selected by a mask (goats) to the LSB end of the word
and all the remaining bits (sheep) to the MSB end of the word, without changing
the order of sheep or goats.

The SAG operation can easily be performed using {\tt bext} (data in {\tt a0} and
mask in {\tt a1}):

\begin{verbatim}
  bext a2, a0, a1
  not a1, a1
  bext a0, a0, a1
  pcnt a1, a1
  ror a0, a0, a1
  or a0, a0, a2
\end{verbatim}

Any arbitrary bit permutation can be implemented in $log_2(\textrm{XLEN})$ SAG
operations.

{\it The Hacker's Delight} describes an optimized standard C implementation of
the SAG operation. Their algorithm takes 254 instructions (for 32 bit) or 340
instructions (for 64 bit) on their reference RISC instruction
set.~\cite[p.~152f,~162f]{Seander05}

% References
% Knuth
% Hackers Delight, Chapter 7-7

\subsection{Using bit-matrix multiply}

{\tt bat[x]or} performs a permutation of bits within each byte when used with a
permutation matrix in {\tt rs2}, and performs a permutation of bytes when used
with a permutation matrix in {\tt rs1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Mirroring and rotating bitboards}

Bitboards are 64-bit bitmasks that are used to represent part of the game state
in chess engines (and other board game AIs). The bits in the bitmask correspond
to squares on a $8 \times 8$ chess board:

\begin{verbatim}
 56 57 58 59 60 61 62 63
 48 49 50 51 52 53 54 55
 40 41 42 43 44 45 46 47
 32 33 34 35 36 37 38 39
 24 25 26 27 28 29 30 31
 16 17 18 19 20 21 22 23
  8  9 10 11 12 13 14 15
  0  1  2  3  4  5  6  7
\end{verbatim}

Many bitboard operations are simple straight-forward operations such as
bitwise-AND, but mirroring and rotating bitboards can take up to 20
instructions on x86.

\subsection{Mirroring bitboards}

Flipping horizontally or vertically can easily done with {\tt grevi}:

\begin{verbatim}
Flip horizontal:
 63 62 61 60 59 58 57 56    RISC-V Bitmanip:
 55 54 53 52 51 50 49 48       rev.b
 47 46 45 44 43 42 41 40
 39 38 37 36 35 34 33 32
 31 30 29 28 27 26 25 24    x86:
 23 22 21 20 19 18 17 16       13 operations
 15 14 13 12 11 10  9  8
  7  6  5  4  3  2  1  0

Flip vertical:
  0  1  2  3  4  5  6  7    RISC-V Bitmanip:
  8  9 10 11 12 13 14 15       rev8
 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31
 32 33 34 35 36 37 38 39    x86:
 40 41 42 43 44 45 46 47       bswap
 48 49 50 51 52 53 54 55
 56 57 58 59 60 61 62 63
\end{verbatim}

Rotating by 180 (flip horizontal and vertical):

\begin{verbatim}
Rotate 180:
  7  6  5  4  3  2  1  0    RISC-V Bitmanip:
 15 14 13 12 11 10  9  8       rev
 23 22 21 20 19 18 17 16
 31 30 29 28 27 26 25 24
 39 38 37 36 35 34 33 32    x86:
 47 46 45 44 43 42 41 40       14 operations
 55 54 53 52 51 50 49 48
 63 62 61 60 59 58 57 56
\end{verbatim}

\subsection{Rotating bitboards}

Using {\tt zip} a bitboard can be transposed easily:
\label{transposebitboard}

\begin{verbatim}
Transpose:
  7 15 23 31 39 47 55 63    RISC-V Bitmanip:
  6 14 22 30 38 46 54 62       zip, zip, zip
  5 13 21 29 37 45 53 61
  4 12 20 28 36 44 52 60
  3 11 19 27 35 43 51 59    x86:
  2 10 18 26 34 42 50 58       18 operations
  1  9 17 25 33 41 49 57
  0  8 16 24 32 40 48 56
\end{verbatim}

A rotation is simply the composition of a flip operation and a transpose
operation. This takes 19 operations on x86~\cite{ChessProg}. With Bitmanip
the rotate operation only takes 4 operations:

\begin{verbatim}
rotate_bitboard:
  rev8 a0, a0
  zip a0, a0
  zip a0, a0
  zip a0, a0
\end{verbatim}

\subsection{Explanation}

The bit indices for a 64-bit word are 6 bits wide. Let $\texttt{i[5:0]}$ be the
index of a bit in the input, and let $\texttt{i$'$[5:0]}$ be the index of the
same bit after the permutation.

As an example, a rotate left shift by $N$ can be expressed using this notation
as $\texttt{i$'$[5:0]} = \texttt{i[5:0]} + N \,\,\, (\textrm{mod 64})$.

The GREV operation with shamt $N$ is $\texttt{i$'$[5:0]} = \texttt{i[5:0]} \textrm{ XOR } N$.

And a SHFL operation corresponds to a rotate left shift by one position of any
contiguous region of $\texttt{i[5:0]}$. For example, {\tt zip} is a left rotate shift
of the entire bit index:

$$\texttt{i$'$[5:0]} = \{ \texttt{i[4:0]},\, \texttt{i[5]} \}$$

And {\tt zip4} performs a left rotate shift on bits {\tt 5:2}:

$$\texttt{i$'$[5:0]} = \{ \texttt{i[4:2]},\, \texttt{i[5]},\, \texttt{i[1:0]} \}$$

In a bitboard, $\texttt{i[2:0]}$ corresponds to the X coordinate of a board position, and
$\texttt{i[5:3]}$ corresponds to the Y coordinate.

Therefore flipping the board horizontally is the same as negating bits $\texttt{i[2:0]}$,
which is the operation performed by {\tt grevi rd, rs, 7} ({\tt rev.b}).

Likewise flipping the board vertically is done by {\tt grevi rd, rs, 56} ({\tt rev8}).

Finally, transposing corresponds by swapping the lower and upper half of $\texttt{i[5:0]}$,
or rotate shifting $\texttt{i[5:0]}$ by 3 positions. This can easily done by rotate shifting the entire
$\texttt{i[5:0]}$ by one bit position ({\tt zip}) three times.

\subsection{Rotating Bitcubes}

Let's define a bitcube as a $4 \times 4 \times 4$ cube with $x=\texttt{i[1:0]}$,
$y=\texttt{i[3:2]}$, and $z=\texttt{i[5:4]}$. Using the same methods as described
above we can easily rotate a bitcube by 90$^\circ$ around the X-, Y-, and Z-axis:

\begin{multicols}{3}
\begin{minipage}{\linewidth}
\begin{verbatim}
rotate_x:
  rev16 a0, a0
  zip4 a0, a0
  zip4 a0, a0
\end{verbatim}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{verbatim}
rotate_y:
  rev.n a0, a0
  zip a0, a0
  zip a0, a0
  zip4 a0, a0
  zip4 a0, a0
\end{verbatim}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{verbatim}
rotate_z:
  rev4.h
  zip.h a0, a0
  zip.h a0, a0
\end{verbatim}
\end{minipage}
\end{multicols}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Manipulating 64x64 Bit Matrices}
\label{bmat64}

The {\tt bmat[x]or} and {\tt bmatflip} instructions operate on 8x8 bit
matrices stored in single 64-bit registers, where each byte of such
a 64-bit value represents one row (column) of a 8x8 bit matrix.

Let's assume we have a 64x64 bit matrix in memory, stored as one row
(column) per 64-bit value. In order to use {\tt bmat[x]or} and
{\tt bmatflip} on such a matrix, we must first convert it into
a 8x8 block matrix of 64 individual 8x8 matrices, each stored in
a 64-bit value. The following function performs this transformation
for a single row (column) of the block matrix in 40 instructions.

\begin{minipage}{\linewidth}
\begin{verbatim}
void conv8x8(const uint64_t x[8], uint64_t y[8])
{
  uint64_t x0_x1_31_00 = _rv64_pack (x[0], x[1]);
  uint64_t x2_x3_31_00 = _rv64_pack (x[2], x[3]);
  uint64_t x4_x5_31_00 = _rv64_pack (x[4], x[5]);
  uint64_t x6_x7_31_00 = _rv64_pack (x[6], x[7]);
  uint64_t x0_x1_63_32 = _rv64_packu(x[0], x[1]);
  uint64_t x2_x3_63_32 = _rv64_packu(x[2], x[3]);
  uint64_t x4_x5_63_32 = _rv64_packu(x[4], x[5]);
  uint64_t x6_x7_63_32 = _rv64_packu(x[6], x[7]);
\end{verbatim}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint64_t x0_x1_31_00_z = _rv64_unzip16(x0_x1_31_00);
  uint64_t x2_x3_31_00_z = _rv64_unzip16(x2_x3_31_00);
  uint64_t x4_x5_31_00_z = _rv64_unzip16(x4_x5_31_00);
  uint64_t x6_x7_31_00_z = _rv64_unzip16(x6_x7_31_00);
  uint64_t x0_x1_63_32_z = _rv64_unzip16(x0_x1_63_32);
  uint64_t x2_x3_63_32_z = _rv64_unzip16(x2_x3_63_32);
  uint64_t x4_x5_63_32_z = _rv64_unzip16(x4_x5_63_32);
  uint64_t x6_x7_63_32_z = _rv64_unzip16(x6_x7_63_32);
\end{verbatim}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint64_t x0_x1_x2_x3_15_00 = _rv64_pack (x0_x1_31_00_z, x2_x3_31_00_z);
  uint64_t x4_x5_x6_x7_15_00 = _rv64_pack (x4_x5_31_00_z, x6_x7_31_00_z);
  uint64_t x0_x1_x2_x3_31_16 = _rv64_packu(x0_x1_31_00_z, x2_x3_31_00_z);
  uint64_t x4_x5_x6_x7_31_16 = _rv64_packu(x4_x5_31_00_z, x6_x7_31_00_z);
  uint64_t x0_x1_x2_x3_47_32 = _rv64_pack (x0_x1_63_32_z, x2_x3_63_32_z);
  uint64_t x4_x5_x6_x7_47_32 = _rv64_pack (x4_x5_63_32_z, x6_x7_63_32_z);
  uint64_t x0_x1_x2_x3_63_48 = _rv64_packu(x0_x1_63_32_z, x2_x3_63_32_z);
  uint64_t x4_x5_x6_x7_63_48 = _rv64_packu(x4_x5_63_32_z, x6_x7_63_32_z);
\end{verbatim}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint64_t x0_x1_x2_x3_15_00_z = _rv64_unzip8(x0_x1_x2_x3_15_00);
  uint64_t x4_x5_x6_x7_15_00_z = _rv64_unzip8(x4_x5_x6_x7_15_00);
  uint64_t x0_x1_x2_x3_31_16_z = _rv64_unzip8(x0_x1_x2_x3_31_16);
  uint64_t x4_x5_x6_x7_31_16_z = _rv64_unzip8(x4_x5_x6_x7_31_16);
  uint64_t x0_x1_x2_x3_47_32_z = _rv64_unzip8(x0_x1_x2_x3_47_32);
  uint64_t x4_x5_x6_x7_47_32_z = _rv64_unzip8(x4_x5_x6_x7_47_32);
  uint64_t x0_x1_x2_x3_63_48_z = _rv64_unzip8(x0_x1_x2_x3_63_48);
  uint64_t x4_x5_x6_x7_63_48_z = _rv64_unzip8(x4_x5_x6_x7_63_48);
\end{verbatim}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{verbatim}
  y[0] = _rv64_pack (x0_x1_x2_x3_15_00_z, x4_x5_x6_x7_15_00_z);
  y[1] = _rv64_packu(x0_x1_x2_x3_15_00_z, x4_x5_x6_x7_15_00_z);
  y[2] = _rv64_pack (x0_x1_x2_x3_31_16_z, x4_x5_x6_x7_31_16_z);
  y[3] = _rv64_packu(x0_x1_x2_x3_31_16_z, x4_x5_x6_x7_31_16_z);
  y[4] = _rv64_pack (x0_x1_x2_x3_47_32_z, x4_x5_x6_x7_47_32_z);
  y[5] = _rv64_packu(x0_x1_x2_x3_47_32_z, x4_x5_x6_x7_47_32_z);
  y[6] = _rv64_pack (x0_x1_x2_x3_63_48_z, x4_x5_x6_x7_63_48_z);
  y[7] = _rv64_packu(x0_x1_x2_x3_63_48_z, x4_x5_x6_x7_63_48_z);
}
\end{verbatim}
\end{minipage}

Each of the 5 blocks in this function only consumes the eight outputs of the
previous block. Therefore 16 registers are sufficient to run this function in
registers only without the need to spill any data on the stack.

Note that this function is its own inverse. Therefore the same function
can be used for the convertion from block matrix form back to row (column)
major form.

A bit 64x64 bit matrix in block matrix form can easily be transposed
by running {\tt bmatflip} (or {\tt zip; zip; zip}) on the blocks of
the matrix and then renaming the individual 64-bit variables.

To multiply 64x64 bit matrices in block matrix form, the matrix-matrix-product
is decomposed in the obvious way in $8 \times 8 \times 8 = 512$ {\tt bmat[x]or}
instructions and $7 \times 8 \times 8 = 448$ {\tt [x]or} instructions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Inverting Xorshift RNGs}
\label{invxorshift}

Xorshift RNGs are a class of fast RNGs for different bit widths. There are 648
Xorshift RNGs for 32 bits, but this is the one that the author of the original
Xorshift RNG paper recommends.~\cite[p. 4]{Xorshift}

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint32_t xorshift32(uint32_t x)
  {
    x ^= x << 13;
    x ^= x >> 17;
    x ^= x << 5;
    return x;
  }
\end{verbatim}
\end{minipage}

This function of course has been designed and selected so it's efficient, even
without special bit-manipulation instructions. So let's look at the inverse
instead. First, the na\"ive form of inverting this function:

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint32_t xorshift32_inv(uint32_t x)
  {
    uint32_t t;
    t = x ^ (x << 5);
    t = x ^ (t << 5);
    t = x ^ (t << 5);
    t = x ^ (t << 5);
    t = x ^ (t << 5);
    x = x ^ (t << 5);
    x = x ^ (x >> 17);
    t = x ^ (x << 13);
    x = x ^ (t << 13);
    return x;
  }
\end{verbatim}
\end{minipage}

This translates to 18 RISC-V instructions, not including the function call overhead.

Obviously the C expression {\tt x \^{} (x >> 17)} is already its own inverse
(because $17 \ge XLEN/2$) and therefore already has an effecient inverse. But the two
other blocks can easily be implemented using a single {\tt clmul} instruction each:

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint32_t xorshift32_inv(uint32_t x)
  {
    x = clmul(x, 0x42108421);
    x = x ^ (x >> 17);
    x = clmul(x, 0x04002001);
    return x;
  }
\end{verbatim}
\end{minipage}

This are 8 RISC-V instructions, including 4 instructions for loading the
constants, but not including the function call overhead.

An optimizing compiler could easily generate the clmul instructions and the magic
constants from the C code for the na\"ive implementation. ({\tt 0x04002001 = (1 << 2*13) | (1 << 13) | 1}
and {\tt 0x42108421 = (1 << 6*5) | (1 << 5*5) | \dots | (1 << 5) | 1})

The obvious remaining question is ``if {\tt clmul(x, 0x42108421)} is the inverse of {\tt x \^{} (x << 5)}, what's
the inverse of {\tt x \^{} (x >> 5)}?'' It's {\tt clmulr(x, 0x84210842)}, where {\tt 0x84210842} is the bit-reversal of {\tt 0x42108421}.

A special case of xorshift is {\tt x \^{} (x >> 1)}, which is a gray encoder. The corresponding gray decoder is {\tt clmulr(x, 0xffffffff)}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Cyclic redundency checks (CRC)}

There are special instructions for performing CRCs using the two most
widespread 32-bit CRC polynomials, CRC-32 and CRC-32C.

CRCs with other polynomials can be computed efficiently using CLMUL.
The following examples are using CRC32Q.

The easiest way of implementing CRC32Q with clmul is using a Barrett reduction.
On RV32:

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint32_t crc32q_simple(const uint32_t *data, int length)
  {
    uint32_t P  = 0x814141AB;  // CRC polynomial (implicit x^32)
    uint32_t mu = 0xFEFF7F62;  // x^64 divided by CRC polynomial
    uint32_t mu1 = 0xFF7FBFB1; // "mu" with leading 1, shifted right by 1 bit
    uint32_t crc = 0;

    for (int i = 0; i < length; i++) {
      crc ^= rev8(data[i]);
      crc = clmulr(crc, mu1);
      crc = clmul(crc, P);
    }

    return crc;
  }
\end{verbatim}
\end{minipage}

The following python code calculates the value of {\tt mu} for a given CRC
polynomial:

\begin{minipage}{\linewidth}
\begin{verbatim}
  def polydiv(dividend, divisor):
      quotient = 0
      while dividend.bit_length() >= divisor.bit_length():
          i = dividend.bit_length() - divisor.bit_length()
          dividend = dividend ^ (divisor << i)
          quotient |= 1 << i
      return quotient

  P = 0x1814141AB
  print("0x%X" % (polydiv(1<<64, P)))   # prints 0x1FEFF7F62
\end{verbatim}
\end{minipage}

A more efficient method would be the following, which processes 64-bit at a
time (RV64):

\begin{minipage}{\linewidth}
\begin{verbatim}
  uint32_t crc32q_fast(const uint64_t *p, int len)
  {
    uint64_t P  = 0x1814141ABLL;   // CRC polynomial
    uint64_t k1 =  0xA1FA6BECLL;   // remainder of x^128 divided by CRC polynomial
    uint64_t k2 =  0x9BE9878FLL;   // remainder of x^96 divided by CRC polynomial
    uint64_t k3 =  0xB1EFC5F6LL;   // remainder of x^64 divided by CRC polynomial
    uint64_t mu = 0x1FEFF7F62LL;   // x^64 divided by CRC polynomial

    uint64_t a0, a1, a2, t1, t2;

    assert(len >= 2);
    a0 = rev8(p[0]);
    a1 = rev8(p[1]);
\end{verbatim}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{verbatim}
    // Main loop: Reduce to 2x 64 bits

    for (const uint64_t *t0 = p+2; t0 != p+len; t0++)
    {
      a2 = rev8(*t0);
      t1 = clmulh(a0, k1);
      t2 = clmul(a0, k1);
      a0 = a1 ^ t1;
      a1 = a2 ^ t2;
    }
\end{verbatim}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{verbatim}
    // Reduce to 64 bit, add 32 bit zero padding

    t1 = clmulh(a0, k2);
    t2 = clmul(a0, k2);

    a0 = (a1 >> 32) ^ t1;
    a1 = (a1 << 32) ^ t2;

    t2 = clmul(a0, k3);
    a1 = a1 ^ t2;
\end{verbatim}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{verbatim}
    // Barrett Reduction

    t1 = clmul(a1 >> 32, mu);
    t2 = clmul(t1 >> 32, P);
    a0 = a1 ^ t2;

    return a0;
  }
\end{verbatim}
\end{minipage}

The main idea is to transform an array of arbitrary length to an array with the
same CRC that's only two 64-bit elements long. (That's the ``Main loop''
portion of above code.)

Then we further reduce it to just 64-bit. And then we use a Barrett
reduction to get the final 32-bit result.

The following python code can be used to calculate the ``magic constants'' {\tt
k1}, {\tt k2}, and {\tt k3}:

\begin{minipage}{\linewidth}
\begin{verbatim}
  def polymod(dividend, divisor):
      quotient = 0
      while dividend.bit_length() >= divisor.bit_length():
          i = dividend.bit_length() - divisor.bit_length()
          dividend = dividend ^ (divisor << i)
          quotient |= 1 << i
      return dividend

  print("0x%X" % (polymod(1<<128, P)))   # prints 0xA1FA6BEC
  print("0x%X" % (polymod(1<< 96, P)))   # prints 0x9BE9878F
  print("0x%X" % (polymod(1<< 64, P)))   # prints 0xB1EFC5F6
\end{verbatim}
\end{minipage}

The above example code is taken from~\cite{Wolf18A}. A more detailed descriptions of
the algorithms employed can be found in~\cite{FastCRC}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Decoding RISC-V Immediates}

The following code snippets decode and sign-extend the immediate from RISC-V
S-type, B-type, J-type, and CJ-type instructions. They are nice ``nothing up my
sleeve''-examples for real-world bit permutations.

\begin{small}
\begin{center}
\begin{tabular}{p{0in}p{0.4in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}p{0.4in}p{0.6in}p{0.4in}p{0.6in}p{0.7in}l}
& & & & & & & & & & \\
                      &
\multicolumn{1}{l}{\instbit{31}} &
\multicolumn{1}{r}{\instbit{27}} &
\instbit{26} &
\instbit{25} &
\multicolumn{1}{l}{\instbit{24}} &
\multicolumn{1}{r}{\instbit{20}} &
\instbitrange{19}{15} &
\instbitrange{14}{12} &
\instbitrange{11}{7} &
\instbitrange{6}{0} \\
\cline{2-11}

&
\multicolumn{4}{|c|}{imm[11:5]} &
\multicolumn{2}{c|}{} &
\multicolumn{1}{c|}{} &
\multicolumn{1}{c|}{} &
\multicolumn{1}{c|}{imm[4:0]} &
\multicolumn{1}{c|}{} & S-type \\
\cline{2-11}

&
\multicolumn{4}{|c|}{imm[12$\vert$10:5]} &
\multicolumn{2}{c|}{} &
\multicolumn{1}{c|}{} &
\multicolumn{1}{c|}{} &
\multicolumn{1}{c|}{imm[4:1$\vert$11]} &
\multicolumn{1}{c|}{} & B-type \\
\cline{2-11}

&
\multicolumn{8}{|c|}{imm[20$\vert$10:1$\vert$11$\vert$19:12]} &
\multicolumn{1}{c|}{} &
\multicolumn{1}{c|}{} & J-type \\
\cline{2-11}

\end{tabular}

\begin{tabular}{p{0in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}p{0.05in}l}
& & & & & & & & & & \\
                      &
\instbit{15} &
\instbit{14} &
\instbit{13} &
\multicolumn{1}{c}{\instbit{12}} &
\instbit{11} &
\instbit{10} &
\instbit{9} &
\instbit{8} &
\instbit{7} &
\instbit{6} &
\multicolumn{1}{c}{\instbit{5}} &
\instbit{4} &
\instbit{3} &
\instbit{2} &
\instbit{1} &
\instbit{0} \\
\cline{2-17}

&
\multicolumn{3}{|c|}{} &
\multicolumn{11}{c|}{imm[11$\vert$4$\vert$9:8$\vert$10$\vert$6$\vert$7$\vert$3:1$\vert$5]} &
\multicolumn{2}{c|}{} & CJ-type \\
\cline{2-17}

\end{tabular}
\end{center}
\end{small}

\begin{multicols}{2}
\begin{minipage}{\linewidth}
\begin{verbatim}
  decode_s:
    li t0, 0xfe000f80
    bext a0, a0, t0
    c.slli a0, 20
    c.srai a0, 20
    ret
\end{verbatim}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{verbatim}
  decode_b:
    li t0, 0xeaa800aa
    rori a0, a0, 8
    grevi a0, a0, 8
    shfli a0, a0, 7
    bext a0, a0, t0
    c.slli a0, 20
    c.srai a0, 19
    ret
\end{verbatim}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{verbatim}
  decode_j:
    li t0, 0x800003ff
    li t1, 0x800ff000
    bext a1, a0, t1
    c.slli a1, 23
    rori a0, a0, 21
    bext a0, a0, t0
    c.slli a0, 12
    c.or a0, a1
    c.srai a0, 11
    ret
\end{verbatim}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{verbatim}
  // variant 1 (with RISC-V Bitmanip)
  decode_cj:
    li t0, 0x28800001
    li t1, 0x000016b8
    li t2, 0xb4e00000
    li t3, 0x4b000000
    bext a1, a0, t1
    bdep a1, a1, t2
    rori a0, a0, 11
    bext a0, a0, t0
    bdep a0, a0, t3
    c.or a0, a1
    c.srai a0, 20
    ret
\end{verbatim}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{verbatim}
  // variant 2 (without RISC-V Bitmanip)
  decode_cj:
    srli a5, a0, 2
    srli a4, a0, 7
    c.andi a4, 16
    slli a3, a0, 3
    c.andi a5, 14
    c.add a5, a4
    andi a3, a3, 32
    srli a4, a0, 1
    c.add a5, a3
    andi a4, a4, 64
    slli a2, a0, 1
    c.add a5, a4
    andi a2, a2, 128
    srli a3, a0, 1
    slli a4, a0, 19
    c.add a5, a2
    andi a3, a3, 768
    c.slli a0, 2
    c.add a5, a3
    andi a0, a0, 1024
    c.srai a4, 31
    c.add a5, a0
    slli a0, a4, 11
    c.add a0, a5
    ret
\end{verbatim}
\end{minipage}
\end{multicols}
